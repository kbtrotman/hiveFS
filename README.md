# hiveFS
A Hive Mind Filesystem

The problem with describing what I'm trying to do here is similar to trying to describe a helicopter to someone who's never seen an airplane or copter. It's new, and not much like anything currently available. It's de-dupe, but that's a small part of it. It's a global filesystem, but it's not meant to be a GF, it's just one as a result of the final goal. There is no moniker to describe it. So, I'll try my best.

Virtualization is huge, and everything is virtualized, even storage to a degree. But is storage really virtualized? We can turn a physical lun into a "virtual lun" but what have we really done to virtualize it beyond just changing how its presented? A hive mind is a full virtualization of the underlying storage, far beyond anything else that's existed. It extends de-dupe from a single filesystem or device across as many hosts as you need/want. What I'm "developing" is hiveFS, but even hiveFS is just a driver to enable the true function here, which is a virtual storage appliance built on either open source software or Enterprise-grade software/hardware. The hive. The pairing between hiveFS and the hive is the ultimate goal. A single appliance that can serve storage to any number of hosts, physical or virtual, limited only by how many disks you can put behind it. The hive software serves up inodes and blocks like a filesystem, reguardless of the configuration under it. Once a host is paired to the hive with certs, there is no other configuration. The only thing that will be stored locally for any machine is the filesystem superblock. The hiveFS software will then point to a remote hive that stores everyone's data using the "hive" version of the hiveFS software. That sounds like a higher-level version of current arrays, you say? Well, an array is presented on a lun level, and the filesystem applied to that lun has to be permanently attached to that machine at a hardware level. It is similar, but yet again, not at all what I'm doing at the same time. There are no luns in the hive. There's only a shared filesystem. The goal is for the hive software to share data to a host, as a CIFS/NFS share, a VMWare object, or object store using S3. Data for a host or object can be easily migratable from one hive to another through a couple clicks, or the back-end disk changed without major changes, all without needing downtime (whereas a lun change is more disruptive). I am seeing a need to draw a diagram to explain this hive software soon. For now, imagine a bycycle spoke-wheel.

First:
The [Hive Applicance] hive software. This device is the hub that controls everything. It would be a blade host, similar to existing array controllers, attached to a huge blob of storage behind it, cpus, memory, etc. The hive will be an installable software image (ISO) built on that blade that helps the admin to automatically configure the hardware into a large pool of storage as one central hive. Once the storage is accessible, rather than serving luns, our hive has a unique purpose, which is to:
   1. Require certificates to prove the host/spoke is a valid hive member.
   2. A configured hive worker will then be given a confirmed spoke on the wheel, which initiates his filesystem at whatever place you mount it to.
      i. We could also say the admin is doing a mkfs here, because in the hive a mkfs linux command will add a host as a spoke on that wheel.
   3. Transfer encrypted filesystem structures to the foreign host, his inode table. and de-duplicated block data.
      i. I should add here that most hosts already have a large block of local storage, so a local cache is prabably a good idea here 
         to increase performance and is feasible using existing disk space. 
   4. Store the remote worker's data as it comes in and as in any filesystem, add links to existing data instead of physical storage.
   5. And store n-number of copies of each block in it's global store. (Replicated/DR'd/ad-nauseum)

Then our remote spokes run the hiveFS software, which is a true filesystem (module in linux) with a local superblock written to a physical disk that points to hive-owned inodes and other filesystem structures. The hive blade can serve a small number of CIFS/NFS/S3 shares or an additional blade/blades can serve larger numbers of shares. So long as the network stays active, the array presents its storage. (The superblock is necessary because I'm not greedy here. I don't want to try and do away with everything that a filesystem is at the moment....my goals are already pretty drastic...you could even say far-fetched, so I'm sticking to what I know and am familiar with until it seems inplausible to continue in that vein.)

I'm very much not a windows developer, but the eventual goal is to support linux and windows, the primary O/S's for virtualization and the biggest footprint for O/S's in general. I wish to add support for VMWare API and S3 direct from the appliance via network/Infiniband. FC is feasable for this, but current technologies are fast and reliable via network, so expenditure in legacy FC is not a priority at the moment.
